version: "3.9"
name: agentflow-v2

networks:
  agentflow:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  qdrant_data:
  ollama_data:
  langfuse_data:
  workspace:

services:

  # ── PostgreSQL + pgvector ───────────────────────────────
  postgres:
    image: pgvector/pgvector:pg16
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-agentflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-agentflow_secret}
      POSTGRES_DB: agentflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    ports: ["5432:5432"]
    networks: [agentflow]
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U ${POSTGRES_USER:-agentflow}"]
      interval: 10s; timeout: 5s; retries: 5

  # ── Redis Stack ─────────────────────────────────────────
  redis:
    image: redis/redis-stack:latest
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD:-redis_secret} --appendonly yes --maxmemory 512mb
    volumes: [redis_data:/data]
    ports: ["6379:6379","8001:8001"]
    networks: [agentflow]
    healthcheck:
      test: ["CMD","redis-cli","-a","${REDIS_PASSWORD:-redis_secret}","ping"]
      interval: 10s; timeout: 5s; retries: 5

  # ── Qdrant Vector DB ────────────────────────────────────
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    volumes: [qdrant_data:/qdrant/storage]
    ports: ["6333:6333","6334:6334"]
    networks: [agentflow]
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334

  # ────────────────────────────────────────────────────────
  # OLLAMA — LOCAL LLM ENGINE
  # GPU version (comment out if no GPU, use CPU below)
  # ────────────────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    volumes: [ollama_data:/root/.ollama]
    ports: ["11434:11434"]
    networks: [agentflow]
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_ORIGINS: "*"
      OLLAMA_KEEP_ALIVE: 30m         # Keep models loaded
      OLLAMA_NUM_PARALLEL: 4         # Concurrent requests
      OLLAMA_MAX_LOADED_MODELS: 3    # Models in VRAM
    # GPU support — comment out if no NVIDIA GPU:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD","curl","-f","http://localhost:11434/api/tags"]
      interval: 15s; timeout: 10s; retries: 5; start_period: 30s

  # CPU-only Ollama (uncomment if no GPU, comment out above)
  # ollama-cpu:
  #   image: ollama/ollama:latest
  #   restart: unless-stopped
  #   volumes: [ollama_data:/root/.ollama]
  #   ports: ["11434:11434"]
  #   networks: [agentflow]
  #   environment:
  #     OLLAMA_HOST: 0.0.0.0
  #     OLLAMA_ORIGINS: "*"
  #   profiles: ["cpu"]

  # ── Ollama model auto-puller ────────────────────────────
  ollama-init:
    image: ollama/ollama:latest
    restart: "no"
    depends_on:
      ollama:
        condition: service_healthy
    volumes: [ollama_data:/root/.ollama]
    networks: [agentflow]
    environment:
      OLLAMA_HOST: ollama:11434
    entrypoint: ["/bin/sh","-c"]
    # Pull default models on first boot
    command:
      - |
        echo "Pulling default models..."
        ollama pull llama3.2:latest
        ollama pull nomic-embed-text
        echo "Done. To pull more models use the Model Hub in the UI."

  # ── LiteLLM Gateway ─────────────────────────────────────
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    restart: unless-stopped
    depends_on:
      postgres: {condition: service_healthy}
    volumes: [./litellm/config.yaml:/app/config.yaml]
    command: ["--config","/app/config.yaml","--port","4000"]
    ports: ["4000:4000"]
    networks: [agentflow]
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-agentflow}:${POSTGRES_PASSWORD:-agentflow_secret}@postgres:5432/litellm
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_secret}@redis:6379
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-agentflow-master}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      GROQ_API_KEY: ${GROQ_API_KEY:-}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf-pk-agentflow}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf-sk-agentflow}
      LANGFUSE_HOST: http://langfuse:3000

  # ── Langfuse Observability ───────────────────────────────
  langfuse:
    image: langfuse/langfuse:latest
    restart: unless-stopped
    depends_on:
      postgres: {condition: service_healthy}
    ports: ["3001:3000"]
    networks: [agentflow]
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-agentflow}:${POSTGRES_PASSWORD:-agentflow_secret}@postgres:5432/langfuse
      NEXTAUTH_URL: http://localhost:3001
      NEXTAUTH_SECRET: ${LANGFUSE_SECRET:-langfuse_secret_change_me}
      SALT: ${LANGFUSE_SALT:-langfuse_salt_change_me}
      LANGFUSE_INIT_ORG_ID: agentflow
      LANGFUSE_INIT_PROJECT_ID: agentflow-project
      LANGFUSE_INIT_PROJECT_NAME: AgentFlow
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf-pk-agentflow}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf-sk-agentflow}
      LANGFUSE_INIT_USER_EMAIL: ${ADMIN_EMAIL:-admin@agentflow.local}
      LANGFUSE_INIT_USER_PASSWORD: ${ADMIN_PASSWORD:-admin123}
      LANGFUSE_INIT_USER_NAME: Admin

  # ── Temporal ────────────────────────────────────────────
  temporal:
    image: temporalio/auto-setup:latest
    restart: unless-stopped
    depends_on:
      postgres: {condition: service_healthy}
    ports: ["7233:7233"]
    networks: [agentflow]
    environment:
      DB: postgres12
      DB_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-agentflow}
      POSTGRES_PWD: ${POSTGRES_PASSWORD:-agentflow_secret}
      POSTGRES_SEEDS: postgres

  temporal-ui:
    image: temporalio/ui:latest
    restart: unless-stopped
    depends_on: [temporal]
    ports: ["8080:8080"]
    networks: [agentflow]
    environment:
      TEMPORAL_ADDRESS: temporal:7233
      TEMPORAL_CORS_ORIGINS: http://localhost:3000

  # ── Open WebUI ──────────────────────────────────────────
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports: ["8888:8080"]
    networks: [agentflow]
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OPENAI_API_BASE_URL: http://litellm:4000/v1
      OPENAI_API_KEY: ${LITELLM_MASTER_KEY:-sk-agentflow-master}
      WEBUI_SECRET_KEY: ${SECRET_KEY:-change_me}
    volumes: [./open-webui:/app/backend/data]

  # ── AgentFlow Backend ───────────────────────────────────
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on:
      postgres: {condition: service_healthy}
      redis: {condition: service_healthy}
      ollama: {condition: service_healthy}
    ports: ["8000:8000"]
    networks: [agentflow]
    volumes: [workspace:/tmp/agentflow_ws]
    environment:
      # Core
      ENVIRONMENT: ${ENVIRONMENT:-production}
      SECRET_KEY: ${SECRET_KEY:-change_me}
      CORS_ORIGINS: http://localhost:3000,http://localhost:5173,http://localhost:8888
      # DB
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-agentflow}:${POSTGRES_PASSWORD:-agentflow_secret}@postgres:5432/agentflow
      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_secret}@redis:6379
      # Ollama (local — always available)
      OLLAMA_URL: http://ollama:11434
      OLLAMA_DEFAULT_MODEL: ${OLLAMA_DEFAULT_MODEL:-llama3.2:latest}
      OLLAMA_DEFAULT_EMBEDDING: ${OLLAMA_DEFAULT_EMBEDDING:-nomic-embed-text}
      OLLAMA_AUTO_PULL: ${OLLAMA_AUTO_PULL:-llama3.2:latest,nomic-embed-text}
      # Remote Ollama hosts (JSON)
      OLLAMA_REMOTE_HOSTS: ${OLLAMA_REMOTE_HOSTS:-[]}
      # Cloud LLMs (optional)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      GROQ_API_KEY: ${GROQ_API_KEY:-}
      # LiteLLM
      LITELLM_URL: http://litellm:4000
      LITELLM_API_KEY: ${LITELLM_MASTER_KEY:-sk-agentflow-master}
      # Observability
      LANGFUSE_HOST: http://langfuse:3000
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf-pk-agentflow}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf-sk-agentflow}
      # Qdrant
      QDRANT_URL: http://qdrant:6333
      # Temporal
      TEMPORAL_URL: temporal:7233
      # Search & Tools
      TAVILY_API_KEY: ${TAVILY_API_KEY:-}
      E2B_API_KEY: ${E2B_API_KEY:-}
      # Default models
      DEFAULT_MODEL_LOCAL: ${OLLAMA_DEFAULT_MODEL:-llama3.2:latest}
      DEFAULT_MODEL_CODING: ${DEFAULT_MODEL_CODING:-qwen2.5-coder:7b}
      DEFAULT_MODEL_RESEARCH: ${DEFAULT_MODEL_RESEARCH:-llama3.1:8b}
      DEFAULT_MODEL_CLOUD: ${DEFAULT_MODEL_CLOUD:-claude-sonnet-4-6}
    healthcheck:
      test: ["CMD","curl","-f","http://localhost:8000/health"]
      interval: 15s; timeout: 10s; retries: 5; start_period: 30s

  # ── AgentFlow Frontend ──────────────────────────────────
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_URL: ${VITE_API_URL:-http://localhost:8000}
        VITE_WS_URL: ${VITE_WS_URL:-ws://localhost:8000}
    restart: unless-stopped
    depends_on:
      backend: {condition: service_healthy}
    ports: ["3000:80"]
    networks: [agentflow]
