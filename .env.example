# ══════════════════════════════════════════════
# AgentFlow v2 — Environment Configuration
# ══════════════════════════════════════════════

# Database
POSTGRES_USER=agentflow
POSTGRES_PASSWORD=agentflow_secret_change_me

# Redis
REDIS_PASSWORD=redis_secret_change_me

# Ollama (primary - connects to local Ollama)
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_DEFAULT_MODEL=llama3.2:latest
OLLAMA_DEFAULT_EMBEDDING=nomic-embed-text
# Additional remote Ollama hosts (JSON array)
# OLLAMA_REMOTE_HOSTS=[{"url":"http://192.168.1.100:11434","name":"Server 1"}]
# Models to auto-pull on startup (comma-separated)
OLLAMA_AUTO_PULL=llama3.2:latest,nomic-embed-text

# Langfuse observability
LANGFUSE_SECRET_KEY=lf-secret-change-me
LANGFUSE_PUBLIC_KEY=lf-public-change-me
LANGFUSE_HOST=http://langfuse:3001

# ── OPTIONAL: Cloud API Keys ──────────────────
# Leave empty to run fully locally with Ollama

ANTHROPIC_API_KEY=
OPENAI_API_KEY=
GEMINI_API_KEY=
GROQ_API_KEY=
TOGETHER_API_KEY=

# Search (optional, fallback to DuckDuckGo if empty)
TAVILY_API_KEY=

# Code execution sandbox (optional, fallback to local)
E2B_API_KEY=

# ── Model defaults ────────────────────────────
DEFAULT_MODEL_LOCAL=llama3.2:latest
DEFAULT_MODEL_CLOUD=claude-haiku-4-5-20251001
DEFAULT_MODEL_CODING=qwen2.5-coder:7b
DEFAULT_MODEL_RESEARCH=llama3.1:8b
DEFAULT_MODEL_FAST=llama3.2:3b

# ── CORS ─────────────────────────────────────
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Mistral AI — https://console.mistral.ai
MISTRAL_API_KEY=

# OpenRouter (100+ models via one key) — https://openrouter.ai/keys
OPENROUTER_API_KEY=
